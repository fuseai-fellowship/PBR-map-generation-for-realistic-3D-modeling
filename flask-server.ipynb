{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f35b2bd",
   "metadata": {
    "papermill": {
     "duration": 0.005593,
     "end_time": "2025-11-06T14:36:39.697749",
     "exception": false,
     "start_time": "2025-11-06T14:36:39.692156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cloning the Metric3D Repository from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d73cff35",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-06T14:36:39.709455Z",
     "iopub.status.busy": "2025-11-06T14:36:39.708674Z",
     "iopub.status.idle": "2025-11-06T14:36:39.712880Z",
     "shell.execute_reply": "2025-11-06T14:36:39.712327Z"
    },
    "papermill": {
     "duration": 0.011767,
     "end_time": "2025-11-06T14:36:39.714502",
     "exception": false,
     "start_time": "2025-11-06T14:36:39.702735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Before running the Kaggle notebook, make sure to select the GPU P100 under the 'Accelerator' option in the 'Session Options'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f8dc4",
   "metadata": {
    "papermill": {
     "duration": 0.005575,
     "end_time": "2025-11-06T14:36:39.724927",
     "exception": false,
     "start_time": "2025-11-06T14:36:39.719352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## all install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf6a649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:36:39.735407Z",
     "iopub.status.busy": "2025-11-06T14:36:39.735189Z",
     "iopub.status.idle": "2025-11-06T14:37:06.140859Z",
     "shell.execute_reply": "2025-11-06T14:37:06.139965Z"
    },
    "papermill": {
     "duration": 26.413529,
     "end_time": "2025-11-06T14:37:06.143192",
     "exception": false,
     "start_time": "2025-11-06T14:36:39.729663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "bigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\r\n",
      "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install mmengine pyngrok plyfile html4vision open3d --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f3a11a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:06.155136Z",
     "iopub.status.busy": "2025-11-06T14:37:06.154813Z",
     "iopub.status.idle": "2025-11-06T14:37:07.160660Z",
     "shell.execute_reply": "2025-11-06T14:37:07.159459Z"
    },
    "papermill": {
     "duration": 1.014353,
     "end_time": "2025-11-06T14:37:07.162843",
     "exception": false,
     "start_time": "2025-11-06T14:37:06.148490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/Texture_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e61f932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:07.174400Z",
     "iopub.status.busy": "2025-11-06T14:37:07.174067Z",
     "iopub.status.idle": "2025-11-06T14:37:08.783780Z",
     "shell.execute_reply": "2025-11-06T14:37:08.782932Z"
    },
    "papermill": {
     "duration": 1.617698,
     "end_time": "2025-11-06T14:37:08.785731",
     "exception": false,
     "start_time": "2025-11-06T14:37:07.168033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Texture_training'...\r\n",
      "remote: Enumerating objects: 222, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (222/222), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (134/134), done.\u001b[K\r\n",
      "remote: Total 222 (delta 91), reused 212 (delta 81), pack-reused 0 (from 0)\u001b[K\r\n",
      "Receiving objects: 100% (222/222), 226.34 KiB | 8.38 MiB/s, done.\r\n",
      "Resolving deltas: 100% (91/91), done.\r\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/Pritipaudel/Texture_training.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939f7829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:08.798455Z",
     "iopub.status.busy": "2025-11-06T14:37:08.798160Z",
     "iopub.status.idle": "2025-11-06T14:37:08.804225Z",
     "shell.execute_reply": "2025-11-06T14:37:08.803252Z"
    },
    "papermill": {
     "duration": 0.01441,
     "end_time": "2025-11-06T14:37:08.805854",
     "exception": false,
     "start_time": "2025-11-06T14:37:08.791444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Texture_training\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/Texture_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5494d",
   "metadata": {
    "papermill": {
     "duration": 0.005219,
     "end_time": "2025-11-06T14:37:08.816432",
     "exception": false,
     "start_time": "2025-11-06T14:37:08.811213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd04639a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:08.828031Z",
     "iopub.status.busy": "2025-11-06T14:37:08.827776Z",
     "iopub.status.idle": "2025-11-06T14:37:11.158510Z",
     "shell.execute_reply": "2025-11-06T14:37:11.157600Z"
    },
    "papermill": {
     "duration": 2.338818,
     "end_time": "2025-11-06T14:37:11.160501",
     "exception": false,
     "start_time": "2025-11-06T14:37:08.821683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'AO' set up to track remote branch 'AO' from 'origin'.\r\n",
      "Switched to a new branch 'AO'\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!git fetch origin\n",
    "!git checkout AO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0f0b84e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:11.173391Z",
     "iopub.status.busy": "2025-11-06T14:37:11.172783Z",
     "iopub.status.idle": "2025-11-06T14:37:12.823588Z",
     "shell.execute_reply": "2025-11-06T14:37:12.822351Z"
    },
    "papermill": {
     "duration": 1.660587,
     "end_time": "2025-11-06T14:37:12.826922",
     "exception": false,
     "start_time": "2025-11-06T14:37:11.166335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Texture_training/training\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/Texture_training/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38cf3c4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:12.849693Z",
     "iopub.status.busy": "2025-11-06T14:37:12.849413Z",
     "iopub.status.idle": "2025-11-06T14:37:20.688762Z",
     "shell.execute_reply": "2025-11-06T14:37:20.688032Z"
    },
    "papermill": {
     "duration": 7.847908,
     "end_time": "2025-11-06T14:37:20.690763",
     "exception": false,
     "start_time": "2025-11-06T14:37:12.842855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from mmengine import Config\n",
    "from mono.model.monodepth_model import DepthModel\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo, upload_file\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba8c60",
   "metadata": {
    "papermill": {
     "duration": 0.005403,
     "end_time": "2025-11-06T14:37:20.702044",
     "exception": false,
     "start_time": "2025-11-06T14:37:20.696641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Installing Essential Dependencies and Resolving Version Conflicts for 3D Monodepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04914e7f",
   "metadata": {
    "papermill": {
     "duration": 0.005277,
     "end_time": "2025-11-06T14:37:20.712828",
     "exception": false,
     "start_time": "2025-11-06T14:37:20.707551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5494ad81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:20.725778Z",
     "iopub.status.busy": "2025-11-06T14:37:20.724897Z",
     "iopub.status.idle": "2025-11-06T14:37:31.347655Z",
     "shell.execute_reply": "2025-11-06T14:37:31.346544Z"
    },
    "papermill": {
     "duration": 10.631351,
     "end_time": "2025-11-06T14:37:31.349727",
     "exception": false,
     "start_time": "2025-11-06T14:37:20.718376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyngrok in /opt/conda/lib/python3.10/site-packages (7.4.1)\r\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.2)\r\n",
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\r\n"
     ]
    }
   ],
   "source": [
    "!pip install  pyngrok\n",
    "!ngrok authtoken 2tCJTC3fdY5MfELkvtwaIM2XyQU_4pjEPWHJPFgPVR74ySQrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be856376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:31.363850Z",
     "iopub.status.busy": "2025-11-06T14:37:31.363542Z",
     "iopub.status.idle": "2025-11-06T14:37:32.168939Z",
     "shell.execute_reply": "2025-11-06T14:37:32.168169Z"
    },
    "papermill": {
     "duration": 0.815053,
     "end_time": "2025-11-06T14:37:32.171286",
     "exception": false,
     "start_time": "2025-11-06T14:37:31.356233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/kaggle/working/Texture_training\"\n",
    "cfg = Config.fromfile(path+'/training/mono/configs/RAFTDecoder/vit.raft5.small.py')\n",
    "\n",
    "# Initialize the DepthModel\n",
    "model = DepthModel(cfg, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ae0034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:32.190039Z",
     "iopub.status.busy": "2025-11-06T14:37:32.189324Z",
     "iopub.status.idle": "2025-11-06T14:37:33.329832Z",
     "shell.execute_reply": "2025-11-06T14:37:33.328955Z"
    },
    "papermill": {
     "duration": 1.15138,
     "end_time": "2025-11-06T14:37:33.331807",
     "exception": false,
     "start_time": "2025-11-06T14:37:32.180427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f243722dd2b4f7dad5bcf069b51cdfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AO_depth_connected_1781.pth:   0%|          | 0.00/154M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded from Hugging Face Hub: /root/.cache/huggingface/hub/models--preetipaudel--pbr_extractor/snapshots/119a9ee54cb87ef5ee8ec9c40922cda5dffc8d88/AO_depth_connected_1781.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/2658076220.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(downloaded_file),strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from Hugging Face Hub!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download the model file from Hugging Face Hub\n",
    "repo_name = \"pbr_extractor\"\n",
    "downloaded_file = hf_hub_download(\n",
    "    repo_id=f\"preetipaudel/{repo_name}\",  # Replace with your Hugging Face username\n",
    "    filename=\"AO_depth_connected_1781.pth\"\n",
    ")\n",
    "print(f\"Model downloaded from Hugging Face Hub: {downloaded_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model and load the state_dict\n",
    "\n",
    "model.load_state_dict(torch.load(downloaded_file),strict=False)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"Model loaded successfully from Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b112f",
   "metadata": {
    "papermill": {
     "duration": 0.006168,
     "end_time": "2025-11-06T14:37:33.344575",
     "exception": false,
     "start_time": "2025-11-06T14:37:33.338407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e992073",
   "metadata": {
    "papermill": {
     "duration": 0.006282,
     "end_time": "2025-11-06T14:37:33.357042",
     "exception": false,
     "start_time": "2025-11-06T14:37:33.350760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4f4ae",
   "metadata": {
    "papermill": {
     "duration": 0.006033,
     "end_time": "2025-11-06T14:37:33.369575",
     "exception": false,
     "start_time": "2025-11-06T14:37:33.363542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Required Libraries and Modules for 3D Monodepth and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a558cc71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:33.383803Z",
     "iopub.status.busy": "2025-11-06T14:37:33.383526Z",
     "iopub.status.idle": "2025-11-06T14:37:33.392719Z",
     "shell.execute_reply": "2025-11-06T14:37:33.391682Z"
    },
    "papermill": {
     "duration": 0.018607,
     "end_time": "2025-11-06T14:37:33.394358",
     "exception": false,
     "start_time": "2025-11-06T14:37:33.375751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/Texture_training/training/mono/utils/do_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/Texture_training/training/mono/utils/do_test.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from mono.utils.unproj_pcd import reconstruct_pcd, save_point_cloud\n",
    "\n",
    "def to_cuda(data: dict):\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            data[k] = v.cuda(non_blocking=True)\n",
    "        if isinstance(v, list) and len(v)>=1 and isinstance(v[0], torch.Tensor):\n",
    "            for i, l_i in enumerate(v):\n",
    "                data[k][i] = l_i.cuda(non_blocking=True)\n",
    "    return data\n",
    "\n",
    "def align_scale(pred: torch.tensor, target: torch.tensor):\n",
    "    mask = target > 0\n",
    "    if torch.sum(mask) > 10:\n",
    "        scale = torch.median(target[mask]) / (torch.median(pred[mask]) + 1e-8)\n",
    "    else:\n",
    "        scale = 1\n",
    "    pred_scaled = pred * scale\n",
    "    return pred_scaled, scale\n",
    "\n",
    "def align_scale_shift(pred: torch.tensor, target: torch.tensor):\n",
    "    mask = target > 0\n",
    "    target_mask = target[mask].cpu().numpy()\n",
    "    pred_mask = pred[mask].cpu().numpy()\n",
    "    if torch.sum(mask) > 10:\n",
    "        scale, shift = np.polyfit(pred_mask, target_mask, deg=1)\n",
    "        if scale < 0:\n",
    "            scale = torch.median(target[mask]) / (torch.median(pred[mask]) + 1e-8)\n",
    "            shift = 0\n",
    "    else:\n",
    "        scale = 1\n",
    "        shift = 0\n",
    "    pred = pred * scale + shift\n",
    "    return pred, scale\n",
    "\n",
    "def align_scale_shift_numpy(pred: np.array, target: np.array):\n",
    "    mask = target > 0\n",
    "    target_mask = target[mask]\n",
    "    pred_mask = pred[mask]\n",
    "    if np.sum(mask) > 10:\n",
    "        scale, shift = np.polyfit(pred_mask, target_mask, deg=1)\n",
    "        if scale < 0:\n",
    "            scale = np.median(target[mask]) / (np.median(pred[mask]) + 1e-8)\n",
    "            shift = 0\n",
    "    else:\n",
    "        scale = 1\n",
    "        shift = 0\n",
    "    pred = pred * scale + shift\n",
    "    return pred, scale\n",
    "\n",
    "\n",
    "def build_camera_model(H : int, W : int, intrinsics : list) -> np.array:\n",
    "    \"\"\"\n",
    "    Encode the camera intrinsic parameters (focal length and principle point) to a 4-channel map. \n",
    "    \"\"\"\n",
    "    fx, fy, u0, v0 = intrinsics\n",
    "    f = (fx + fy) / 2.0\n",
    "    # principle point location\n",
    "    x_row = np.arange(0, W).astype(np.float32)\n",
    "    x_row_center_norm = (x_row - u0) / W\n",
    "    x_center = np.tile(x_row_center_norm, (H, 1)) # [H, W]\n",
    "\n",
    "    y_col = np.arange(0, H).astype(np.float32) \n",
    "    y_col_center_norm = (y_col - v0) / H\n",
    "    y_center = np.tile(y_col_center_norm, (W, 1)).T # [H, W]\n",
    "\n",
    "    # FoV\n",
    "    fov_x = np.arctan(x_center / (f / W))\n",
    "    fov_y = np.arctan(y_center / (f / H))\n",
    "\n",
    "    cam_model = np.stack([x_center, y_center, fov_x, fov_y], axis=2)\n",
    "    return cam_model\n",
    "\n",
    "def resize_for_input(image, output_shape, intrinsic, canonical_shape, to_canonical_ratio):\n",
    "    \"\"\"\n",
    "    Resize the input.\n",
    "    Resizing consists of two processed, i.e. 1) to the canonical space (adjust the camera model); 2) resize the image while the camera model holds. Thus the\n",
    "    label will be scaled with the resize factor.\n",
    "    \"\"\"\n",
    "    padding = [123.675, 116.28, 103.53]\n",
    "    h, w, _ = image.shape\n",
    "    resize_ratio_h = output_shape[0] / canonical_shape[0]\n",
    "    resize_ratio_w = output_shape[1] / canonical_shape[1]\n",
    "    to_scale_ratio = min(resize_ratio_h, resize_ratio_w)\n",
    "\n",
    "    resize_ratio = to_canonical_ratio * to_scale_ratio\n",
    "\n",
    "    reshape_h = int(resize_ratio * h)\n",
    "    reshape_w = int(resize_ratio * w)\n",
    "\n",
    "    pad_h = max(output_shape[0] - reshape_h, 0)\n",
    "    pad_w = max(output_shape[1] - reshape_w, 0)\n",
    "    pad_h_half = int(pad_h / 2)\n",
    "    pad_w_half = int(pad_w / 2)\n",
    "\n",
    "    # resize\n",
    "    image = cv2.resize(image, dsize=(reshape_w, reshape_h), interpolation=cv2.INTER_LINEAR)\n",
    "    # padding\n",
    "    image = cv2.copyMakeBorder(\n",
    "        image, \n",
    "        pad_h_half, \n",
    "        pad_h - pad_h_half, \n",
    "        pad_w_half, \n",
    "        pad_w - pad_w_half, \n",
    "        cv2.BORDER_CONSTANT, \n",
    "        value=padding)\n",
    "    \n",
    "    # Resize, adjust principle point\n",
    "    intrinsic[2] = intrinsic[2] * to_scale_ratio\n",
    "    intrinsic[3] = intrinsic[3] * to_scale_ratio\n",
    "\n",
    "    cam_model = build_camera_model(reshape_h, reshape_w, intrinsic)\n",
    "    cam_model = cv2.copyMakeBorder(\n",
    "        cam_model, \n",
    "        pad_h_half, \n",
    "        pad_h - pad_h_half, \n",
    "        pad_w_half, \n",
    "        pad_w - pad_w_half, \n",
    "        cv2.BORDER_CONSTANT, \n",
    "        value=-1)\n",
    "\n",
    "    pad=[pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half]\n",
    "    label_scale_factor=1/to_scale_ratio\n",
    "    return image, cam_model, pad, label_scale_factor\n",
    "\n",
    "\n",
    "def get_prediction(\n",
    "    model: torch.nn.Module,\n",
    "    input: torch.tensor,\n",
    "    cam_model: torch.tensor,\n",
    "    pad_info: torch.tensor,\n",
    "    scale_info: torch.tensor,\n",
    "    gt_depth: torch.tensor,\n",
    "    normalize_scale: float,\n",
    "    ori_shape: list=[],\n",
    "):\n",
    "\n",
    "    data = dict(\n",
    "        input=input,\n",
    "        cam_model=cam_model,\n",
    "    )\n",
    "    #pred_depth, confidence, output_dict = model.module.inference(data)\n",
    "    prediction = model.inference(data)\n",
    "    pred_depth=prediction[\"prediction\"]\n",
    "    \n",
    "    pred_depth = pred_depth.squeeze()\n",
    "    pred_depth = pred_depth[pad_info[0] : pred_depth.shape[0] - pad_info[1], pad_info[2] : pred_depth.shape[1] - pad_info[3]]\n",
    "    # confidence = confidence.squeeze()\n",
    "    # confidence = confidence[pad_info[0] : confidence.shape[0] - pad_info[1], pad_info[2] : confidence.shape[1] - pad_info[3]]\n",
    "    if gt_depth is not None:\n",
    "        resize_shape = gt_depth.shape\n",
    "    elif ori_shape != []:\n",
    "        resize_shape = ori_shape\n",
    "    else:\n",
    "        resize_shape = pred_depth.shape\n",
    "\n",
    "    pred_depth = torch.nn.functional.interpolate(pred_depth[None, None, :, :], resize_shape, mode='bilinear').squeeze() # to original size\n",
    "    pred_depth = pred_depth * normalize_scale / scale_info\n",
    "    if gt_depth is not None:\n",
    "        pred_depth_scale, scale = align_scale(pred_depth, gt_depth)\n",
    "    else:\n",
    "        pred_depth_scale = None\n",
    "        scale = None\n",
    "    confidence=[]\n",
    "    return pred_depth, pred_depth_scale, scale, prediction, confidence\n",
    "\n",
    "def transform_test_data_scalecano(rgb, intrinsic, data_basic):\n",
    "    \"\"\"\n",
    "    Pre-process the input for forwarding. Employ `label scale canonical transformation.'\n",
    "        Args:\n",
    "            rgb: input rgb image. [H, W, 3]\n",
    "            intrinsic: camera intrinsic parameter, [fx, fy, u0, v0]\n",
    "            data_basic: predefined canonical space in configs.\n",
    "    \"\"\"\n",
    "    canonical_space = data_basic['canonical_space']\n",
    "    forward_size = data_basic.crop_size\n",
    "    mean = torch.tensor([123.675, 116.28, 103.53]).float()[:, None, None]\n",
    "    std = torch.tensor([58.395, 57.12, 57.375]).float()[:, None, None]\n",
    "\n",
    "    # BGR to RGB\n",
    "    rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    ori_h, ori_w, _ = rgb.shape\n",
    "    ori_focal = (intrinsic[0] + intrinsic[1]) / 2\n",
    "    canonical_focal = canonical_space['focal_length']\n",
    "\n",
    "    cano_label_scale_ratio = canonical_focal / ori_focal\n",
    "\n",
    "    canonical_intrinsic = [\n",
    "        intrinsic[0] * cano_label_scale_ratio,\n",
    "        intrinsic[1] * cano_label_scale_ratio,\n",
    "        intrinsic[2],\n",
    "        intrinsic[3],\n",
    "    ]\n",
    "\n",
    "    # resize\n",
    "    rgb, cam_model, pad, resize_label_scale_ratio = resize_for_input(rgb, forward_size, canonical_intrinsic, [ori_h, ori_w], 1.0)\n",
    "\n",
    "    # label scale factor\n",
    "    label_scale_factor = cano_label_scale_ratio * resize_label_scale_ratio\n",
    "\n",
    "    rgb = torch.from_numpy(rgb.transpose((2, 0, 1))).float()\n",
    "    rgb = torch.div((rgb - mean), std)\n",
    "    rgb = rgb[None, :, :, :].cuda()\n",
    "    #rgb = rgb[None, :, :, :]\n",
    "    \n",
    "    cam_model = torch.from_numpy(cam_model.transpose((2, 0, 1))).float()\n",
    "    cam_model = cam_model[None, :, :, :].cuda()\n",
    "    #cam_model = cam_model[None, :, :, :]\n",
    "    cam_model_stacks = [\n",
    "        torch.nn.functional.interpolate(cam_model, size=(cam_model.shape[2]//i, cam_model.shape[3]//i), mode='bilinear', align_corners=False)\n",
    "        for i in [2, 4, 8, 16, 32]\n",
    "    ]\n",
    "    return rgb, cam_model_stacks, pad, label_scale_factor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "342bed8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:33.407849Z",
     "iopub.status.busy": "2025-11-06T14:37:33.407371Z",
     "iopub.status.idle": "2025-11-06T14:37:34.996978Z",
     "shell.execute_reply": "2025-11-06T14:37:34.996313Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 1.598498,
     "end_time": "2025-11-06T14:37:34.998953",
     "exception": false,
     "start_time": "2025-11-06T14:37:33.400455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "import cupy\n",
    "import sys\n",
    "try:\n",
    "    from mmcv.utils import Config, DictAction\n",
    "except:\n",
    "    from mmengine import Config, DictAction\n",
    "\n",
    "import glob\n",
    "# from mono.utils.comm import init_env\n",
    "# from mono.model.monodepth_model import get_configured_monodepth_model\n",
    "# from mono.utils.running import load_ckpt\n",
    "from mono.utils.do_test import transform_test_data_scalecano, get_prediction\n",
    "# from mono.utils.custom_data import load_from_annos, load_data\n",
    "\n",
    "# from mono.utils.avg_meter import MetricAverageMeter\n",
    "# from mono.utils.visualization import save_val_imgs, create_html, save_raw_imgs, save_normal_val_imgs\n",
    "# import cv2\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# from PIL import Image, ExifTags\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from mono.utils.unproj_pcd import reconstruct_pcd, save_point_cloud #, ply_to_obj\n",
    "from mono.utils.transform import gray_to_colormap\n",
    "from mono.utils.visualization import vis_surface_normal\n",
    "# import gradio as gr\n",
    "# import plotly.graph_objects as go\n",
    "from pyngrok import ngrok\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d297a328",
   "metadata": {
    "papermill": {
     "duration": 0.006183,
     "end_time": "2025-11-06T14:37:35.012286",
     "exception": false,
     "start_time": "2025-11-06T14:37:35.006103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99986780",
   "metadata": {
    "papermill": {
     "duration": 0.006105,
     "end_time": "2025-11-06T14:37:35.024517",
     "exception": false,
     "start_time": "2025-11-06T14:37:35.018412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main Code Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aca0b138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:35.038409Z",
     "iopub.status.busy": "2025-11-06T14:37:35.037932Z",
     "iopub.status.idle": "2025-11-06T14:37:36.060908Z",
     "shell.execute_reply": "2025-11-06T14:37:36.059877Z"
    },
    "papermill": {
     "duration": 1.032226,
     "end_time": "2025-11-06T14:37:36.062938",
     "exception": false,
     "start_time": "2025-11-06T14:37:35.030712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/processed_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f702131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:36.077546Z",
     "iopub.status.busy": "2025-11-06T14:37:36.077273Z",
     "iopub.status.idle": "2025-11-06T14:37:36.176968Z",
     "shell.execute_reply": "2025-11-06T14:37:36.176313Z"
    },
    "papermill": {
     "duration": 0.109133,
     "end_time": "2025-11-06T14:37:36.178693",
     "exception": false,
     "start_time": "2025-11-06T14:37:36.069560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cfg_large = Config.fromfile('/kaggle/working/Metric3D/mono/configs/HourglassDecoder/vit.raft5.large.py')\n",
    "# model_large = get_configured_monodepth_model(cfg_large, )\n",
    "# model_large, _,  _, _ = load_ckpt('/kaggle/working/Metric3D/weight/metric_depth_vit_large_800k.pth', model_large, strict_match=False)\n",
    "# model_large.eval()\n",
    "\n",
    "# cfg_small = Config.fromfile('/kaggle/working/Metric3D/mono/configs/HourglassDecoder/vit.raft5.small.py')\n",
    "# model_small = get_configured_monodepth_model(cfg_small, )\n",
    "# model_small, _,  _, _ = load_ckpt('/kaggle/working/Metric3D/weight/metric_depth_vit_small_800k.pth', model_small, strict_match=False)\n",
    "\n",
    "from mono.utils.do_test import transform_test_data_scalecano\n",
    "device = \"cuda\"\n",
    "# model_large.to(device)\n",
    "model.to(device)\n",
    "\n",
    "def predict_depth_normal(img, model_selection=\"vit-small\", fx=1000.0, fy=1000.0, state_cache={}):\n",
    "\n",
    "    \n",
    "    \n",
    "    if img is None:\n",
    "        return None, None, None, None, state_cache, \"Please upload an image and wait for the upload to complete.\"\n",
    "\n",
    "\n",
    "    cv_image = np.array(img) \n",
    "    img = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "    intrinsic = [fx, fy, img.shape[1]/2, img.shape[0]/2]\n",
    "    rgb_input, cam_models_stacks, pad, label_scale_factor = transform_test_data_scalecano(img, intrinsic, cfg.data_basic)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_depth, pred_depth_scale, scale, output, confidence = get_prediction(\n",
    "                    model = model,\n",
    "                    input = rgb_input,\n",
    "                    cam_model = cam_models_stacks,\n",
    "                    pad_info = pad,\n",
    "                    scale_info = label_scale_factor,\n",
    "                    gt_depth = None,\n",
    "                    normalize_scale = cfg.data_basic.depth_range[1],\n",
    "                    ori_shape=[img.shape[0], img.shape[1]],\n",
    "                )\n",
    "        print(output['prediction_normal'].shape)\n",
    "        \n",
    "        pred_normal = output['prediction_normal'][:, :3, :, :] \n",
    "        H, W = pred_normal.shape[2:]\n",
    "        pred_normal = pred_normal[:, :, pad[0]:H-pad[1], pad[2]:W-pad[3]]\n",
    "\n",
    "    pred_depth = pred_depth.squeeze().cpu().numpy()\n",
    "    pred_depth[pred_depth<0] = 0\n",
    "    pred_color = gray_to_colormap(pred_depth)\n",
    "\n",
    "    pred_normal = torch.nn.functional.interpolate(pred_normal, [img.shape[0], img.shape[1]], mode='bilinear').squeeze()\n",
    "    pred_normal = pred_normal.permute(1,2,0)\n",
    "    pred_color_normal = vis_surface_normal(pred_normal)\n",
    "    pred_normal = pred_normal.cpu().numpy()\n",
    "    \n",
    "    # Storing depth and normal map in state for potential 3D reconstruction\n",
    "    state_cache['depth'] = pred_depth\n",
    "    state_cache['normal'] = pred_normal\n",
    "    state_cache['img'] = img\n",
    "    state_cache['intrinsic'] = intrinsic\n",
    "    state_cache['confidence'] = confidence \n",
    "\n",
    "    # save depth and normal map to .npy file\n",
    "    if 'save_dir' not in state_cache:\n",
    "        cache_id = np.random.randint(0, 100000000000)\n",
    "        while osp.exists(f'recon_cache/{cache_id:08d}'):\n",
    "            cache_id = np.random.randint(0, 100000000000)\n",
    "        state_cache['save_dir'] = f'recon_cache/{cache_id:08d}'\n",
    "        os.makedirs(state_cache['save_dir'], exist_ok=True)\n",
    "    depth_file = f\"{state_cache['save_dir']}/depth.npy\"\n",
    "    normal_file = f\"{state_cache['save_dir']}/normal.npy\"\n",
    "    np.save(depth_file, pred_depth)\n",
    "    np.save(normal_file, pred_normal)\n",
    "\n",
    "    ##formatted = (output * 255 / np.max(output)).astype('uint8')\n",
    "    img = Image.fromarray(pred_color)\n",
    "    img_normal = Image.fromarray(pred_color_normal)\n",
    "    return img, output, img_normal, normal_file, state_cache, \"Success!\"\n",
    "\n",
    "def get_camera(img):\n",
    "    if img is None:\n",
    "        return None, None, None, \"Please upload an image and wait for the upload to complete.\"\n",
    "    try:\n",
    "        exif = img.getexif()\n",
    "        exif.update(exif.get_ifd(ExifTags.IFD.Exif))\n",
    "    except:\n",
    "        exif = {}\n",
    "    sensor_width = exif.get(ExifTags.Base.FocalPlaneYResolution, None)\n",
    "    sensor_height = exif.get(ExifTags.Base.FocalPlaneXResolution, None)\n",
    "    focal_length = exif.get(ExifTags.Base.FocalLength, None)\n",
    "    \n",
    "    # convert sensor size to mm, see https://photo.stackexchange.com/questions/40865/how-can-i-get-the-image-sensor-dimensions-in-mm-to-get-circle-of-confusion-from\n",
    "    w, h = img.size\n",
    "    sensor_width = w / sensor_width * 25.4 if sensor_width is not None else None\n",
    "    sensor_height = h / sensor_height * 25.4 if sensor_height is not None else None\n",
    "    focal_length = focal_length * 1.0 if focal_length is not None else None\n",
    "\n",
    "    message = \"Success!\"\n",
    "    if focal_length is None:\n",
    "        message = \"Focal length not found in EXIF. Please manually input.\"\n",
    "    elif sensor_width is None and sensor_height is None:\n",
    "        sensor_width = 16\n",
    "        sensor_height = h / w * sensor_width\n",
    "        message = f\"Sensor size not found in EXIF. Using {sensor_width}x{sensor_height:.2f} mm as default.\"\n",
    "\n",
    "    return sensor_width, sensor_height, focal_length, message\n",
    "\n",
    "def get_intrinsic(img, sensor_width, sensor_height, focal_length):\n",
    "    if img is None:\n",
    "        return None, None, \"Please upload an image and wait for the upload to complete.\"\n",
    "    if sensor_width is None or sensor_height is None or focal_length is None:\n",
    "        return 1000, 1000, \"Insufficient information. Try detecting camera first or use default 1000 for fx and fy.\"\n",
    "    if sensor_width == 0 or sensor_height == 0 or focal_length == 0:\n",
    "        return 1000, 1000, \"Insufficient information. Try detecting camera first or use default 1000 for fx and fy.\"\n",
    "    \n",
    "    # calculate focal length in pixels\n",
    "    w, h = img.size\n",
    "    fx = w / sensor_width * focal_length if sensor_width is not None else None\n",
    "    fy = h / sensor_height * focal_length if sensor_height is not None else None\n",
    "\n",
    "    # if fx is None:\n",
    "    #     return fy, fy, \"Sensor width not provided, using fy for both fx and fy\"\n",
    "    # if fy is None:\n",
    "    #     return fx, fx, \"Sensor height not provided, using fx for both fx and fy\"\n",
    "\n",
    "    return fx, fy, \"Success!\"\n",
    "\n",
    "\n",
    "def unprojection_pcd(state_cache):\n",
    "    depth_map = state_cache.get('depth', None)\n",
    "    normal_map = state_cache.get('normal', None)\n",
    "    img = state_cache.get('img', None)\n",
    "    intrinsic = state_cache.get('intrinsic', None)\n",
    "\n",
    "    if depth_map is None or img is None:\n",
    "        return None, \"Please predict depth and normal first.\"\n",
    "    \n",
    "    # # downsample/upsample the depth map to confidence map size\n",
    "    # confidence = state_cache.get('confidence', None)\n",
    "    # if confidence is not None:\n",
    "    #     H, W = confidence.shape\n",
    "    #     # intrinsic[0] *= W / depth_map.shape[1]\n",
    "    #     # intrinsic[1] *= H / depth_map.shape[0]\n",
    "    #     # intrinsic[2] *= W / depth_map.shape[1]\n",
    "    #     # intrinsic[3] *= H / depth_map.shape[0]\n",
    "    #     depth_map = cv2.resize(depth_map, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "    #     img = cv2.resize(img, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    #     # filter out depth map by confidence\n",
    "    #     mask = confidence.cpu().numpy() > 0\n",
    "\n",
    "    # downsample the depth map if too large\n",
    "    if depth_map.shape[0] > 1080:\n",
    "        scale = 1080 / depth_map.shape[0]\n",
    "        depth_map = cv2.resize(depth_map, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        intrinsic = [intrinsic[0]*scale, intrinsic[1]*scale, intrinsic[2]*scale, intrinsic[3]*scale,intrinsic[4]*scale]\n",
    "    \n",
    "    if 'save_dir' not in state_cache:\n",
    "        cache_id = np.random.randint(0, 100000000000)\n",
    "        while osp.exists(f'recon_cache/{cache_id:08d}'):\n",
    "            cache_id = np.random.randint(0, 100000000000)\n",
    "        state_cache['save_dir'] = f'recon_cache/{cache_id:08d}'\n",
    "        os.makedirs(state_cache['save_dir'], exist_ok=True)\n",
    "\n",
    "    pcd_ply = f\"{state_cache['save_dir']}/output.ply\"\n",
    "    pcd_obj = pcd_ply.replace(\".ply\", \".obj\")\n",
    "    \n",
    "    pcd = reconstruct_pcd(depth_map, intrinsic[0], intrinsic[1], intrinsic[2], intrinsic[3])\n",
    "    # if mask is not None:\n",
    "    #     pcd_filtered = pcd[mask]\n",
    "    #     img_filtered = img[mask]\n",
    "    pcd_filtered = pcd.reshape(-1, 3)\n",
    "    img_filtered = img.reshape(-1, 3)\n",
    "\n",
    "    save_point_cloud(pcd_filtered, img_filtered, pcd_ply, binary=False)\n",
    "    # ply_to_obj(pcd_ply, pcd_obj)\n",
    "\n",
    "    # downsample the point cloud for visualization\n",
    "    num_samples = 250000\n",
    "    if pcd_filtered.shape[0] > num_samples:\n",
    "        indices = np.random.choice(pcd_filtered.shape[0], num_samples, replace=False)\n",
    "        pcd_downsampled = pcd_filtered[indices]\n",
    "        img_downsampled = img_filtered[indices]\n",
    "    else:\n",
    "        pcd_downsampled = pcd_filtered\n",
    "        img_downsampled = img_filtered\n",
    "\n",
    "    # plotly show\n",
    "    color_str = np.array([f\"rgb({r},{g},{b})\" for b,g,r in img_downsampled])\n",
    "    data=[go.Scatter3d(\n",
    "        x=pcd_downsampled[:,0],\n",
    "        y=pcd_downsampled[:,1],\n",
    "        z=pcd_downsampled[:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1,\n",
    "            color=color_str,\n",
    "            opacity=0.8,\n",
    "        )\n",
    "    )]\n",
    "    layout = go.Layout(\n",
    "        margin=dict(l=0, r=0, b=0, t=0),\n",
    "        scene=dict(\n",
    "            camera = dict(\n",
    "                eye=dict(x=0, y=0, z=-1),\n",
    "                up=dict(x=0, y=-1, z=0)\n",
    "            ),\n",
    "            xaxis=dict(showgrid=False, showticklabels=False, visible=False),\n",
    "            yaxis=dict(showgrid=False, showticklabels=False, visible=False),\n",
    "            zaxis=dict(showgrid=False, showticklabels=False, visible=False),\n",
    "        )\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    return fig, pcd_ply, \"Success!\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Use Ngrok to expose the Gradio app\n",
    "# public_url = ngrok.connect(7866)\n",
    "# print(f\"Public URL: {public_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0e2f5c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:36.192200Z",
     "iopub.status.busy": "2025-11-06T14:37:36.191908Z",
     "iopub.status.idle": "2025-11-06T14:37:44.601536Z",
     "shell.execute_reply": "2025-11-06T14:37:44.600599Z"
    },
    "papermill": {
     "duration": 8.418866,
     "end_time": "2025-11-06T14:37:44.603749",
     "exception": false,
     "start_time": "2025-11-06T14:37:36.184883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask-cors\r\n",
      "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Requirement already satisfied: flask>=0.9 in /opt/conda/lib/python3.10/site-packages (from flask-cors) (3.0.3)\r\n",
      "Requirement already satisfied: Werkzeug>=0.7 in /opt/conda/lib/python3.10/site-packages (from flask-cors) (3.0.4)\r\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from flask>=0.9->flask-cors) (3.1.4)\r\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from flask>=0.9->flask-cors) (2.2.0)\r\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from flask>=0.9->flask-cors) (8.1.7)\r\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from flask>=0.9->flask-cors) (1.8.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from Werkzeug>=0.7->flask-cors) (2.1.5)\r\n",
      "Downloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\r\n",
      "Installing collected packages: flask-cors\r\n",
      "Successfully installed flask-cors-6.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install flask-cors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36080c",
   "metadata": {
    "papermill": {
     "duration": 0.006228,
     "end_time": "2025-11-06T14:37:44.616808",
     "exception": false,
     "start_time": "2025-11-06T14:37:44.610580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c61c12",
   "metadata": {
    "papermill": {
     "duration": 0.009495,
     "end_time": "2025-11-06T14:37:44.636217",
     "exception": false,
     "start_time": "2025-11-06T14:37:44.626722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5639ce3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:44.662068Z",
     "iopub.status.busy": "2025-11-06T14:37:44.661003Z",
     "iopub.status.idle": "2025-11-06T14:37:45.783770Z",
     "shell.execute_reply": "2025-11-06T14:37:45.782629Z"
    },
    "papermill": {
     "duration": 1.137584,
     "end_time": "2025-11-06T14:37:45.785667",
     "exception": false,
     "start_time": "2025-11-06T14:37:44.648083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import open3d as o3d\n",
    "def unprojection_pcd(state_cache):\n",
    "    depth_map = state_cache.get('depth', None)\n",
    "    normal_map = state_cache.get('normal', None)\n",
    "    img = state_cache.get('img', None)\n",
    "    intrinsic = state_cache.get('intrinsic', None)\n",
    "\n",
    "    if depth_map is None or img is None:\n",
    "        return None, \"Please predict depth and normal first.\"\n",
    "    \n",
    "    if depth_map.shape[0] > 1080:\n",
    "        scale = 1080 / depth_map.shape[0]\n",
    "        depth_map = cv2.resize(depth_map, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        intrinsic = [intrinsic[0]*scale, intrinsic[1]*scale, intrinsic[2]*scale, intrinsic[3]*scale]\n",
    "    \n",
    "    if 'save_dir' not in state_cache:\n",
    "        cache_id = np.random.randint(0, 100000000000)\n",
    "        while osp.exists(f'recon_cache/{cache_id:08d}'):\n",
    "            cache_id = np.random.randint(0, 100000000000)\n",
    "        state_cache['save_dir'] = f'recon_cache/{cache_id:08d}'\n",
    "        os.makedirs(state_cache['save_dir'], exist_ok=True)\n",
    "\n",
    "    pcd_ply = f\"{state_cache['save_dir']}/output.ply\"\n",
    "    pcd_obj = pcd_ply.replace(\".ply\", \".obj\")\n",
    "    print(depth_map.shape,type(depth_map))\n",
    "    pcd = reconstruct_pcd(depth_map, intrinsic[0], intrinsic[2], intrinsic[3])\n",
    "    # if mask is not None:\n",
    "    #     pcd_filtered = pcd[mask]\n",
    "    #     img_filtered = img[mask]\n",
    "    print(pcd.shape,img.shape)\n",
    "    pcd_filtered = pcd.reshape(-1, 3)\n",
    "    img_filtered = img.reshape(-1, 3)\n",
    "    # print(pcd_filtered.size,img_filtered.size)\n",
    "\n",
    "    # save_point_cloud(pcd_filtered, img_filtered, pcd_ply, binary=False)\n",
    "    # ply_to_obj(pcd_ply, pcd_obj)\n",
    "\n",
    "    # downsample the point cloud for visualization\n",
    "    num_samples = 250000\n",
    "    if pcd_filtered.shape[0] > num_samples:\n",
    "        indices = np.random.choice(pcd_filtered.shape[0], num_samples, replace=False)\n",
    "        pcd_downsampled = pcd_filtered[indices]\n",
    "        img_downsampled = img_filtered[indices]\n",
    "    else:\n",
    "        pcd_downsampled = pcd_filtered\n",
    "        img_downsampled = img_filtered\n",
    "\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pcd_downsampled)\n",
    "    # Normalize color values to [0, 1] if they are in [0, 255]\n",
    "    pcd.colors = o3d.utility.Vector3dVector(img_downsampled / 255.0)\n",
    "    \n",
    "    # Save as a PLY file (Blender can import this)\n",
    "    o3d.io.write_point_cloud(pcd_ply, pcd)\n",
    "    \n",
    "    return  pcd_ply, \"Success!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c9846a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:45.801130Z",
     "iopub.status.busy": "2025-11-06T14:37:45.800269Z",
     "iopub.status.idle": "2025-11-06T14:37:53.916747Z",
     "shell.execute_reply": "2025-11-06T14:37:53.915643Z"
    },
    "papermill": {
     "duration": 8.126414,
     "end_time": "2025-11-06T14:37:53.919162",
     "exception": false,
     "start_time": "2025-11-06T14:37:45.792748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyngrok in /opt/conda/lib/python3.10/site-packages (7.4.1)\r\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "# Install pyngrok if not already installed\n",
    "!pip install pyngrok\n",
    "\n",
    "# Import ngrok\n",
    "from pyngrok import ngrok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "934ba9f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T14:37:53.935071Z",
     "iopub.status.busy": "2025-11-06T14:37:53.934313Z",
     "iopub.status.idle": "2025-11-06T14:37:54.703797Z",
     "shell.execute_reply": "2025-11-06T14:37:54.702512Z"
    },
    "papermill": {
     "duration": 0.779697,
     "end_time": "2025-11-06T14:37:54.706157",
     "exception": true,
     "start_time": "2025-11-06T14:37:53.926460",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:  authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\n",
      "ERROR:  Run multiple endpoints at the same time from a single agent by defining them in your agent configuration file and running `ngrok start --all`.\n",
      "ERROR:  Read more about the agent configuration file: https://ngrok.com/docs/agent/config/ \n",
      "ERROR:  You can view your current agent sessions in the dashboard: https://dashboard.ngrok.com/agents. Upgrade to a paid plan to remove this limit:\n",
      "ERROR:  https://dashboard.ngrok.com/billing/choose-a-plan\r\n",
      "ERROR:  \r\n",
      "ERROR:  ERR_NGROK_108\r\n",
      "ERROR:  https://ngrok.com/docs/errors/err_ngrok_108\r\n",
      "ERROR:  \n"
     ]
    },
    {
     "ename": "PyngrokNgrokError",
     "evalue": "The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nRun multiple endpoints at the same time from a single agent by defining them in your agent configuration file and running `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/agent/config/ \\nYou can view your current agent sessions in the dashboard: https://dashboard.ngrok.com/agents. Upgrade to a paid plan to remove this limit:\\nhttps://dashboard.ngrok.com/billing/choose-a-plan\\r\\n\\r\\nERR_NGROK_108\\r\\n.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflask_cors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CORS\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m public_url \u001b[38;5;241m=\u001b[39m \u001b[43mngrok\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5002\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPublic URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpublic_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove_ply\u001b[39m(src_path, dest_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/processed_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyngrok/ngrok.py:387\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    383\u001b[0m _upgrade_legacy_params(pyngrok_config, options)\n\u001b[1;32m    385\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpening tunnel named: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 387\u001b[0m api_url \u001b[38;5;241m=\u001b[39m \u001b[43mget_ngrok_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mapi_url\n\u001b[1;32m    389\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating tunnel with options: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    391\u001b[0m tunnel \u001b[38;5;241m=\u001b[39m NgrokTunnel(api_request(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/tunnels\u001b[39m\u001b[38;5;124m\"\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    392\u001b[0m                                  timeout\u001b[38;5;241m=\u001b[39mpyngrok_config\u001b[38;5;241m.\u001b[39mrequest_timeout),\n\u001b[1;32m    393\u001b[0m                      pyngrok_config, api_url)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyngrok/ngrok.py:203\u001b[0m, in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    199\u001b[0m     pyngrok_config \u001b[38;5;241m=\u001b[39m conf\u001b[38;5;241m.\u001b[39mget_default()\n\u001b[1;32m    201\u001b[0m install_ngrok(pyngrok_config)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyngrok/process.py:271\u001b[0m, in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_process_running(pyngrok_config\u001b[38;5;241m.\u001b[39mngrok_path):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _current_processes[pyngrok_config\u001b[38;5;241m.\u001b[39mngrok_path]\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_start_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyngrok_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyngrok/process.py:447\u001b[0m, in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    444\u001b[0m kill_process(pyngrok_config\u001b[38;5;241m.\u001b[39mngrok_path)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrok_process\u001b[38;5;241m.\u001b[39mstartup_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe ngrok process errored on start: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mngrok_process\u001b[38;5;241m.\u001b[39mstartup_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    448\u001b[0m                             ngrok_process\u001b[38;5;241m.\u001b[39mlogs,\n\u001b[1;32m    449\u001b[0m                             ngrok_process\u001b[38;5;241m.\u001b[39mstartup_error)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PyngrokNgrokError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe ngrok process was unable to start.\u001b[39m\u001b[38;5;124m\"\u001b[39m, ngrok_process\u001b[38;5;241m.\u001b[39mlogs)\n",
      "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nRun multiple endpoints at the same time from a single agent by defining them in your agent configuration file and running `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/agent/config/ \\nYou can view your current agent sessions in the dashboard: https://dashboard.ngrok.com/agents. Upgrade to a paid plan to remove this limit:\\nhttps://dashboard.ngrok.com/billing/choose-a-plan\\r\\n\\r\\nERR_NGROK_108\\r\\n."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from flask import Flask, request, jsonify, send_file\n",
    "from PIL import Image\n",
    "from flask_cors import CORS\n",
    "\n",
    "import shutil\n",
    "public_url = ngrok.connect(5002)\n",
    "print(f\"Public URL: {public_url}\") \n",
    "\n",
    "\n",
    "def move_ply(src_path, dest_folder=\"/kaggle/working/processed_folder\"):\n",
    "\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "    \n",
    "    # Get the base name (file or folder name)\n",
    "    item_name = os.path.basename(src_path)\n",
    "    dest_path = os.path.join(dest_folder, item_name)\n",
    "    \n",
    "    # Move the file or folder\n",
    "    shutil.move(src_path, dest_path)\n",
    "    return dest_path\n",
    "\n",
    "def save_pil_image(image,filename,folder=\"/kaggle/working/processed_folder\"):\n",
    "    \"\"\"Save a tensor as a PNG image.\"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    image.save(os.path.join(folder, filename))\n",
    "    \n",
    "\n",
    "def save_tensor_image(tensor, filename, folder=\"/kaggle/working/processed_folder\"):\n",
    "    \"\"\"Save a tensor as a PNG image.\"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    # Convert tensor to PIL image\n",
    "    transform = transforms.ToPILImage()\n",
    "    image = transform(tensor.cpu().detach())\n",
    "    \n",
    "    # Save the image\n",
    "    image.save(os.path.join(folder, filename))\n",
    "\n",
    "def process_image(image):\n",
    "    \"\"\"Mock function to generate processed images.\"\"\"\n",
    "    img, output, img_normal, normal_file, state_cache, message = predict_depth_normal(image)\n",
    "    pcd_ply, pcd_message = unprojection_pcd(state_cache)\n",
    "    move_ply(pcd_ply)\n",
    "    # if message != \"success\":\n",
    "    #     return \"error\"\n",
    "    \n",
    "    # Save images\n",
    "    save_tensor_image(output[\"roughness\"].squeeze(0).squeeze(0), \"roughness.png\")\n",
    "    save_tensor_image(output[\"ao\"].squeeze(0).squeeze(0), \"ambient_occlusion.png\")\n",
    "    save_pil_image(img_normal, \"normal_map.png\")\n",
    "    save_pil_image(img, \"depth.png\")\n",
    "\n",
    "    \n",
    "    return \"success\"\n",
    "\n",
    "app = Flask(__name__)\n",
    "PROCESSED_FOLDER = \"/kaggle/working/processed_folder\"\n",
    "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
    "@app.route(\"/process\", methods=[\"POST\"])\n",
    "def process():\n",
    "    if \"image\" not in request.files:\n",
    "        return jsonify({\"error\": \"No image uploaded\"}), 400\n",
    "    \n",
    "    image_file = request.files[\"image\"]\n",
    "    image = Image.open(image_file)\n",
    "    save_path = os.path.join(\"/kaggle/working\", image_file.filename)\n",
    "    \n",
    "    # Save the image\n",
    "    image.save(save_path)\n",
    "    \n",
    "    result = process_image(image)\n",
    "    if result == \"error\":\n",
    "        return jsonify({\"error\": \"Processing failed\"}), 500\n",
    "    \n",
    "    zip_buffer = io.BytesIO()\n",
    "    with zipfile.ZipFile(zip_buffer, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for filename in os.listdir(PROCESSED_FOLDER):\n",
    "            if filename.endswith(\".png\") or filename.endswith(\".ply\"):  # Include depth, normal, and PLY\n",
    "                zipf.write(os.path.join(PROCESSED_FOLDER, filename), filename)\n",
    "    zip_buffer.seek(0)\n",
    "    \n",
    "    return send_file(zip_buffer, mimetype=\"application/zip\", as_attachment=True, download_name=\"processed_images.zip\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5002, debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9612c1b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 42780,
     "sourceId": 75676,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2204890,
     "sourceId": 3684316,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 79.232608,
   "end_time": "2025-11-06T14:37:56.543701",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-06T14:36:37.311093",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a5dbc389db44e45a96884dfb8fda80c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18548fe732bf491d8b5046ec5f9399ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4f243722dd2b4f7dad5bcf069b51cdfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f1658ca55906486390b588e8b30de6b9",
        "IPY_MODEL_e7ec0ce99ac047b9adcdf033cfc6d337",
        "IPY_MODEL_e74aae31871a492bb02b839375fd5169"
       ],
       "layout": "IPY_MODEL_61cb51a041a04ad7ba680ecf796094e6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "61cb51a041a04ad7ba680ecf796094e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a821ecd782684ccf9ebcbf096a6739b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d991acd4343c48558ab0426f33aa2804": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e74aae31871a492bb02b839375fd5169": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_18548fe732bf491d8b5046ec5f9399ed",
       "placeholder": "",
       "style": "IPY_MODEL_f155ba639d014bae825c3db4fe8a2f2f",
       "tabbable": null,
       "tooltip": null,
       "value": "154M/154M[00:00&lt;00:00,325MB/s]"
      }
     },
     "e7ec0ce99ac047b9adcdf033cfc6d337": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a821ecd782684ccf9ebcbf096a6739b4",
       "max": 154213138.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d991acd4343c48558ab0426f33aa2804",
       "tabbable": null,
       "tooltip": null,
       "value": 154213138.0
      }
     },
     "ee73857f80e143c1935304f830a29f26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f155ba639d014bae825c3db4fe8a2f2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f1658ca55906486390b588e8b30de6b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0a5dbc389db44e45a96884dfb8fda80c",
       "placeholder": "",
       "style": "IPY_MODEL_ee73857f80e143c1935304f830a29f26",
       "tabbable": null,
       "tooltip": null,
       "value": "AO_depth_connected_1781.pth:100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
